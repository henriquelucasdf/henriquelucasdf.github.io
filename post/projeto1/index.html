<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Projeto 1 - Previsão de vendas utilizando um modelo de regressão multivariada. | Henrique Lucas</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Introdução Este projeto consiste em uma previsão de vendas para a cadeia de drogarias alemã Rossmann. O objetivo final é prever quanto cada uma de suas lojas irá faturar em uma janela temporal de 6 semanas. Os resultados serão disponibilizados através de um Bot no Telegram, onde os possíveis interessados poderão acessá-los de qualquer smartphone.
Este projeto foi baseado no curso &ldquo;Data Science em Produção&rdquo;, do cientista de dados brasileiro Meigarom Lopes.">
    <meta name="generator" content="Hugo 0.78.1" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="https://henriquelucasdf.github.io/dist/css/app.4fc0b62e4b82c997bb0041217cd6b979.css" rel="stylesheet">
    

    
      <link rel="stylesheet" href="https://henriquelucasdf.github.io/css/custom.css">
    

    
      

    

    
    
    <meta property="og:title" content="Projeto 1 - Previsão de vendas utilizando um modelo de regressão multivariada." />
<meta property="og:description" content="Introdução Este projeto consiste em uma previsão de vendas para a cadeia de drogarias alemã Rossmann. O objetivo final é prever quanto cada uma de suas lojas irá faturar em uma janela temporal de 6 semanas. Os resultados serão disponibilizados através de um Bot no Telegram, onde os possíveis interessados poderão acessá-los de qualquer smartphone.
Este projeto foi baseado no curso &ldquo;Data Science em Produção&rdquo;, do cientista de dados brasileiro Meigarom Lopes." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://henriquelucasdf.github.io/post/projeto1/" />
<meta property="article:published_time" content="2020-11-07T10:58:08-03:00" />
<meta property="article:modified_time" content="2020-11-07T10:58:08-03:00" />
<meta itemprop="name" content="Projeto 1 - Previsão de vendas utilizando um modelo de regressão multivariada.">
<meta itemprop="description" content="Introdução Este projeto consiste em uma previsão de vendas para a cadeia de drogarias alemã Rossmann. O objetivo final é prever quanto cada uma de suas lojas irá faturar em uma janela temporal de 6 semanas. Os resultados serão disponibilizados através de um Bot no Telegram, onde os possíveis interessados poderão acessá-los de qualquer smartphone.
Este projeto foi baseado no curso &ldquo;Data Science em Produção&rdquo;, do cientista de dados brasileiro Meigarom Lopes.">
<meta itemprop="datePublished" content="2020-11-07T10:58:08-03:00" />
<meta itemprop="dateModified" content="2020-11-07T10:58:08-03:00" />
<meta itemprop="wordCount" content="4277">



<meta itemprop="keywords" content="Previsão de Vendas,Modelo em Produção,XGBoost,RandomForest,GPU Acceleration,Bot telegram," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Projeto 1 - Previsão de vendas utilizando um modelo de regressão multivariada."/>
<meta name="twitter:description" content="Introdução Este projeto consiste em uma previsão de vendas para a cadeia de drogarias alemã Rossmann. O objetivo final é prever quanto cada uma de suas lojas irá faturar em uma janela temporal de 6 semanas. Os resultados serão disponibilizados através de um Bot no Telegram, onde os possíveis interessados poderão acessá-los de qualquer smartphone.
Este projeto foi baseado no curso &ldquo;Data Science em Produção&rdquo;, do cientista de dados brasileiro Meigarom Lopes."/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  
  <header class="cover bg-top" style="background-image: url('https://henriquelucasdf.github.io/images/Rossmann.jpg');">
    <div class="pb3-m pb6-l bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://henriquelucasdf.github.io/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Henrique Lucas
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://henriquelucasdf.github.io/post/" title="Projetos page">
              Projetos
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://henriquelucasdf.github.io/about/" title="Sobre mim page">
              Sobre mim
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://henriquelucasdf.github.io/contact/" title="Contato page">
              Contato
            </a>
          </li>
          
        </ul>
      
      








<a href="https://github.com/henriquelucasdf" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>








    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Projeto 1 - Previsão de vendas utilizando um modelo de regressão multivariada.</h1>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        PROJETOS
      </aside>
      




  <div id="sharing" class="mt3">

    
    <a href="https://www.facebook.com/sharer.php?u=https://henriquelucasdf.github.io/post/projeto1/" class="facebook no-underline" aria-label="share on Facebook">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

    </a>

    
    
    <a href="https://twitter.com/share?url=https://henriquelucasdf.github.io/post/projeto1/&amp;text=Projeto%201%20-%20Previs%c3%a3o%20de%20vendas%20utilizando%20um%20modelo%20de%20regress%c3%a3o%20multivariada." class="twitter no-underline" aria-label="share on Twitter">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

    </a>

    
    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://henriquelucasdf.github.io/post/projeto1/&amp;title=Projeto%201%20-%20Previs%c3%a3o%20de%20vendas%20utilizando%20um%20modelo%20de%20regress%c3%a3o%20multivariada." class="linkedin no-underline" aria-label="share on LinkedIn">
      <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

    </a>
  </div>


      <h1 class="f1 athelas mt3 mb1">Projeto 1 - Previsão de vendas utilizando um modelo de regressão multivariada.</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2020-11-07T10:58:08-03:00">November 7, 2020</time>

      
      
        <span class="f6 mv4 dib tracked"> - 21 minutes read</span>
        <span class="f6 mv4 dib tracked"> - 4277 words</span>
      
    </header>
    <div class="nested-copy-line-height lh-copy times f4 nested-links nested-img mid-gray pr4-l w-90-l"><h2 id="introdução">Introdução</h2>
<p>Este projeto consiste em uma previsão de vendas para a cadeia de drogarias alemã Rossmann. O objetivo final é prever quanto cada uma de suas lojas irá faturar em uma janela temporal de 6 semanas. Os resultados serão disponibilizados através de um Bot no Telegram, onde os possíveis interessados poderão acessá-los de qualquer smartphone.</p>
<p>Este projeto foi baseado no curso <a href="https://sejaumdatascientist.com/como-ser-um-data-scientist/">&ldquo;Data Science em Produção&rdquo;</a>, do cientista de dados brasileiro Meigarom Lopes.</p>
<p>Os dados para a realização desse projeto foram obtidos de uma competição do <a href="https://www.kaggle.com/c/rossmann-store-sales">Kaggle</a>.</p>
<p><strong>Conteúdo:</strong></p>
<ul>
<li><a href="#10-prepara%C3%A7%C3%A3o-dos-dados">1.0 Preparação dos Dados</a></li>
<li><a href="#20-feature-engineering">2.0 Feature Engineering</a></li>
<li><a href="#30-an%C3%A1lise-explorat%C3%B3ria-de-dados-eda">3.0 Análise Exploratória de Dados (EDA)</a></li>
<li><a href="#40-prepara%C3%A7%C3%A3o-dos-dados">4.0 Preparação dos Dados</a></li>
<li><a href="#50-sele%C3%A7%C3%A3o-de-features">5.0 Seleção de Features</a></li>
<li><a href="#60-elabora%C3%A7%C3%A3o-dos-modelos-de-machine-learning">6.0 Elaboração dos Modelos de Machine Learning</a></li>
<li><a href="#70-otimiza%C3%A7%C3%A3o-dos-hiperpar%C3%A2metros">7.0 Otimização dos Hiperparâmetros</a></li>
<li><a href="#80-valida%C3%A7%C3%A3o-com-os-dados-de-teste">8.0 Validação com os Dados de Teste</a></li>
<li><a href="#90-deploy-do-modelo-em-produ%C3%A7%C3%A3o">9.0 Deploy do Modelo em Produção</a></li>
</ul>
<h2 id="10-preparação-dos-dados">1.0 Preparação dos Dados</h2>
<h4 id="11-renomeando-as-colunas">1.1. Renomeando as colunas</h4>
<p>Inicialmente, para padronizar os dados as serem trabalhados, utilizou-se a biblioteca Inflection para converter as colunas para &ldquo;snakecase&rdquo;.</p>
<h4 id="12-dimensão-dos-dados">1.2. Dimensão dos Dados</h4>
<ul>
<li>Dataset de treino: (969.264 x 18)</li>
<li>Dataset de teste: (47.945 x 18)</li>
<li>Datas:
<ul>
<li>Início: 01-01-2013.</li>
<li>Fim: 31-07-2015.</li>
</ul>
</li>
</ul>
<h4 id="13-tipo-de-dados">1.3. Tipo de dados</h4>
<p>As variáveis iniciais do problema são descritas abaixo:</p>
<ul>
<li>
<p>Variáveis Numéricas:</p>
<ul>
<li><strong>store:</strong> Contém a identificação de cada loja.</li>
<li><strong>day_of_week:</strong> Dia da semana; 1 = Segunda, 7 = Domingo.</li>
<li><strong>sales:</strong> A venda obtida em um determinado dia.</li>
<li><strong>customers:</strong> O número de clientes no dia.</li>
<li><strong>open:</strong> Indica se a loja está aberta (1) ou fechada (0).</li>
<li><strong>promo:</strong> Indica se a loja está com alguma promoção no dia.</li>
<li><strong>school_holiday:</strong> Indica se a loja foi afetada pelo fechamento de escolas.</li>
<li><strong>competition_distance:</strong> distância em metros até a loja concorrente mais próxima.</li>
<li><strong>promo:</strong> Indica se uma loja está fazendo uma promoção naquele dia.</li>
<li><strong>promo2:</strong> Promo2 é uma promoção contínua e consecutiva para algumas lojas: 0 = loja não participa, 1 = loja participa.</li>
</ul>
</li>
<li>
<p>Variáveis categóricas:</p>
<ul>
<li><strong>date:</strong> A data de cada venda</li>
<li><strong>state_holiday:</strong> Indica se há um feriado.
<ul>
<li>a = public holiday</li>
<li>b = Easter holiday</li>
<li>c = Christmas</li>
<li>0 = Nenhum</li>
</ul>
</li>
<li><strong>store_type:</strong> Separa a loja em 4 tipos: a, b, c, d.</li>
<li><strong>assortment:</strong> Indica os tipos de produtos na loja.
<ul>
<li>a = básico</li>
<li>b = extra</li>
<li>c = estendido</li>
</ul>
</li>
<li><strong>competition_open_since (month e year):</strong> Fornece o ano e mês aproximados em que o concorrente mais próximo foi aberto.</li>
<li><strong>promo2_since (week e year):</strong> Descreve o ano e a semana em que a loja começou a participar da Promo2.</li>
<li><strong>promo_interval:</strong> Descreve os intervalos consecutivos em que a Promo2 é iniciada, nomeando os meses em que a promoção é reiniciada.</li>
</ul>
</li>
</ul>
<h4 id="14-verificando-nas">1.4 Verificando NAs:</h4>
<p>Utilizando a biblioteca Pandas, obteve-se as seguintes quantidades de NAs para cada variável:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Variável</th>
<th style="text-align:center">Nº de NAs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">competition_distance</td>
<td style="text-align:center">2.642</td>
</tr>
<tr>
<td style="text-align:left">competition_open_since_month</td>
<td style="text-align:center">323.348</td>
</tr>
<tr>
<td style="text-align:left">competition_open_since_year</td>
<td style="text-align:center">323.348</td>
</tr>
<tr>
<td style="text-align:left">promo2_since_week</td>
<td style="text-align:center">508.031</td>
</tr>
<tr>
<td style="text-align:left">promo2_since_year</td>
<td style="text-align:center">508.031</td>
</tr>
<tr>
<td style="text-align:left">promo_interval</td>
<td style="text-align:center">508.031</td>
</tr>
</tbody>
</table>
<p>*(As outras colunas não possuem NAs)</p>
<h4 id="15-preenchendo-os-valores-na">1.5 Preenchendo os valores NA</h4>
<p>Como há um elevado número de NAs,  simplesmente excluir as linhas em que estes ocorrem não é uma boa abordagem. Além disso, como os modelos de Machine Learning em geral não lidam bem com a sua presença, é necessário imputar valores onde estes ocorrem.</p>
<p>Sendo assim, para as colunas abaixo seguiu-se a seguinte abordagem:</p>
<ul>
<li><strong>competition_distance:</strong> Sendo NA, significa provavelmente que aquela loja não possui nenhum concorrente próximo naquela data. Assim, foi atribuído um valor absurdamente alto para substituir cada NA.</li>
<li><strong>promo_interval:</strong> Os valores NA foram preenchidos com zero, já que o NA indica que a loja não está participando da &ldquo;promo2&rdquo;. Ressalta-se que esta variável foi utilizada para derivar a coluna &ldquo;is_promo&rdquo;, indicando se uma loja está participando ou não da promoção. A coluna promo_interval será excluída posteriormente.</li>
</ul>
<p>Como as colunas <strong>&ldquo;competition_open_since_month&rdquo;, &ldquo;competition_open_since_year&rdquo;, &ldquo;promo2_since_week&rdquo; e &ldquo;promo2_since_year&rdquo;</strong> se referem a datas, não existe uma abordagem ideal para imputar valores nas linhas contendo NAs. Desta forma, a coluna &ldquo;date&rdquo; foi utilizada como suporte para preencher estes valores.</p>
<p><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
<h2 id="20-feature-engineering">2.0 Feature Engineering</h2>
<p>Nesta etapa, é necessário criarmos novas variáveis para o nosso modelo, buscando a maximização de sua performance. Uma boa abordagem para realizar esta tarefa é observar as necessidades da empresa em questão, elaborando diversas hipóteses de negócio que guiarão a obtenção das features e serão posteriormente validadas na Análise Exploratória de Dados.</p>
<h4 id="21-criação-de-hipóteses-de-negócio">2.1 Criação de hipóteses de negócio</h4>
<table>
<thead>
<tr>
<th style="text-align:left">As seguintes hipóteses foram elaboradas:</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">H1. Lojas com maior sortimento deveriam vender mais.</td>
</tr>
<tr>
<td style="text-align:left">H2. Lojas com competidores mais próximos deveriam vender menos.</td>
</tr>
<tr>
<td style="text-align:left">H3. Lojas com competidores a mais tempo deveriam vender mais.</td>
</tr>
<tr>
<td style="text-align:left">H4. Lojas com promoções ativas por mais tempo nos produtos deveriam vender mais.</td>
</tr>
<tr>
<td style="text-align:left">H5. Lojas com mais promoções consecutivas deveriam vender mais.</td>
</tr>
<tr>
<td style="text-align:left">H6. Lojas abertas durante o Natal deveriam vender mais.</td>
</tr>
<tr>
<td style="text-align:left">H7. Lojas deveriam vender mais ao longo dos anos.</td>
</tr>
<tr>
<td style="text-align:left">H8. Lojas deveriam vender mais no segundo semestre do ano.</td>
</tr>
<tr>
<td style="text-align:left">H9. Lojas deveriam vender mais depois do dia 10 de cada mês.</td>
</tr>
<tr>
<td style="text-align:left">H10. Lojas deveriam vender menos aos finais de semana.</td>
</tr>
<tr>
<td style="text-align:left">H11. Lojas deveriam vender menos durante os feriados escolares.</td>
</tr>
</tbody>
</table>
<h4 id="22-obtenção-de-features">2.2 Obtenção de features</h4>
<ul>
<li>
<p>A variável <strong>&ldquo;date&rdquo;</strong> foi utilizada para criar novas colunas com dados temporais:</p>
<ul>
<li><strong>&ldquo;promo2_time_week&rdquo; e &ldquo;promo2_time_month&rdquo;:</strong> indicam a quanto tempo as lojas estão realizando promoções consecutivas.</li>
<li><strong>&ldquo;competition_since_month&rdquo;:</strong> indica a quantos meses aquela loja possui competição.</li>
<li><strong>&ldquo;day&rdquo;,  &ldquo;month&rdquo;, &ldquo;year_week&rdquo; e &ldquo;is_weekday&rdquo;:</strong> foram criadas para melhor detalhar a variável <strong>&ldquo;date&rdquo;</strong></li>
</ul>
</li>
<li>
<p><strong>&ldquo;state_holiday&rdquo; e &ldquo;assortment&rdquo;:</strong> essas variáveis eram descritas apenas por letras. Optou-se por substituir essas letras pelas palavras as quais elas se referiam.</p>
</li>
</ul>
<h4 id="23-filtragem-de-variáveis">2.3 Filtragem de variáveis</h4>
<p>Nesta etapa deve-se checar as restrições de negócio que devem ser aplicadas no projeto, filtrando linhas e colunas que não atendem aos requisitos.</p>
<ul>
<li>
<p><strong>Filtragem de linhas:</strong> entradas com lojas fechadas ou com lojas sem faturamento foram excluídas, já que ambos os casos são irrelevantes para a previsão de vendas.</p>
</li>
<li>
<p><strong>Filtragem de colunas:</strong> variáveis que não estarão disponíveis no momento da previsão devem ser excluídas. A coluna <strong>&ldquo;customers&rdquo;</strong> poderia ser prevista, mas seria necessário um outro projeto de regressão apenas para isso. Desta forma, forma excluídas as colunas <strong>&ldquo;customers&rdquo;, &ldquo;open&rdquo;, &ldquo;promo_interval&rdquo; e &ldquo;month_map&rdquo;</strong>.</p>
</li>
</ul>
<h2 id="30-análise-exploratória-de-dados-eda">3.0 Análise Exploratória de Dados (EDA)</h2>
<p>Este tópico é um dos mais importantes de todo o projeto. O principal objetivo da Análise Exploratória de Dados é conhecer melhor os dados e gerar diversos insights para o negócio. Esta é uma etapa fundamental para ter uma noção de quais são as variáveis importantes do modelo e como cada uma delas se comporta em relação à variável objetivo (vendas).
Para melhor visualização, esta etapa foi dividida em 3 partes:</p>
<ol>
<li><strong>Análise Univariada:</strong> visa verificar a distribuição das features e seus comportamentos. Aqui será visualizado a distribuição das features e um trabalho de estatística descritiva será realizado.</li>
<li><strong>Análise Bivariada:</strong> aqui o objetivo é verificar como cada variável se comporta em relação à variável target. Nesta análise será realizada a validação das hipóteses de negócio criada na seção <a href="#20-feature-engineering">2.0</a></li>
<li><strong>Análise Multivariada:</strong> visa verificar se existe informações repetidas entre as features. Como modelos de Machine Learning performam melhor em modelos menos complexos, reduzir a dimensionalidade dos dados é uma boa prática.</li>
</ol>
<h4 id="31-análise-univariada">3.1 Análise Univariada</h4>
<ul>
<li>
<p><strong>Variável resposta (Sales):</strong> A distribuição se assemelha à distribuição de Poisson, leptocúrtica e com assimetria positiva.</p>
<p><img src="https://henriquelucasdf.github.io/images/uni_sales.png" alt=""></p>
</li>
<li>
<p><strong>Estatística descritiva das variáveis numéricas:</strong></p>
<p><img src="https://henriquelucasdf.github.io/images/uni_num.png" alt=""></p>
<ul>
<li><strong>Conclusões:</strong>
<ul>
<li>O máximo de vendas em uma dia foi US$ 41.551,00, sendo a média de vendas diária de US$ 6.955,96;</li>
<li>O desvio padrão das vendas é elevado, indicando uma grande variação ao longo do tempo.</li>
<li>Existem lojas com competidores a no mínimo 20 metros de distância.</li>
</ul>
</li>
</ul>
<p>Visualizando as distribuições:</p>
<p><img src="https://henriquelucasdf.github.io/images/num_vendas.png" alt=""></p>
<ul>
<li><strong>Conclusões:</strong>
<ul>
<li>Nenhuma variável possui distribuição normal.</li>
<li>A distribuição de dados entre as lojas é uniforme.</li>
<li>Há mais dados no primeiro semestre, indicando maior numero de vendas.</li>
<li>A maior parte das lojas possuem competidores próximos.</li>
<li>A maioria dos competidores são recentes,</li>
<li>Há poucas vendas nos domingos.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Variáveis categóricas:</strong></p>
<p><img src="https://henriquelucasdf.github.io/images/uni_cat.png" alt=""></p>
<ul>
<li><strong>Conclusões:</strong>
<ul>
<li><strong>&ldquo;public_holidays&rdquo;</strong> é o feriado mais comum. <strong>&ldquo;easter_holiday&rdquo;</strong> apresenta a menor curtose em relação às vendas.</li>
<li>O tipo mais comum de loja é o tipo &ldquo;a&rdquo;. Todos os tipos possuem distribuição de vendas semelhantes, com exceção das lojas tipo &ldquo;b&rdquo;.</li>
<li><strong>&ldquo;assortment&rdquo;</strong> do tipo &ldquo;basic&rdquo; é o mais comum, seguido pelo tipo &ldquo;extended&rdquo;. Esses dois tipos possuem vendas distribuídas semelhantemente, enquanto o tipo &ldquo;extra&rdquo; possui distribuição mais platicúrtica.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="32-análise-bivariada">3.2 Análise Bivariada</h4>
<ul>
<li>
<p><strong>Variáveis Categóricas x Vendas:</strong></p>
<p><img src="https://henriquelucasdf.github.io/images/cat_sales.png" alt=""></p>
<ul>
<li><strong>Conclusões:</strong>
<ul>
<li>Nos feriados estaduais do tipo &ldquo;b&rdquo; as lojas tendem a vender mais, enquanto nos do tipo &ldquo;a&rdquo;, menos.</li>
<li>Lojas do tipo &ldquo;b&rdquo; tendem a vender mais; Do tipo a vendem menos.</li>
<li>Lojas com assortment do tipo &ldquo;b&rdquo; apresentam maiores vendas, enquanto o do tipo &ldquo;c&rdquo; menores.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Validação das hipóteses de negócio:</strong> Nesta seção será avaliado o comportamento das variáveis independentes em relação à variável alvo. Utilizando as hipóteses como um guia, iremos gerar insights para o negócio e analisar as features envolvidas.</p>
<ul>
<li>
<p><strong>H1. Lojas com maior sortimento deveriam vender mais:</strong></p>
<ul>
<li><strong>Verdadeiro.</strong> Na média, as lojas com mais sortimento (extra) vendem mais.</li>
</ul>
<p><img src="https://henriquelucasdf.github.io/images/h1_0.png" alt=""></p>
<p><img src="https://henriquelucasdf.github.io/images/h1_l.png" alt=""></p>
</li>
<li>
<p><strong>H2. Lojas com competidores mais próximos deveriam vender menos:</strong></p>
<ul>
<li><strong>Falso.</strong> Não há correlação entre a média de vendas e a distância dos competidores</li>
</ul>
<p><img src="https://henriquelucasdf.github.io/images/h2_0.png" alt=""></p>
</li>
<li>
<p><strong>H3. Lojas com competidores a mais tempo deveriam vender mais:</strong></p>
<ul>
<li><strong>Falso.</strong> Lojas com competidores a mais tempo tendem a vender, em média, menos.</li>
</ul>
<p><img src="https://henriquelucasdf.github.io/images/he.png" alt=""></p>
</li>
<li>
<p><strong>H4. Lojas com promoções ativas por mais tempo nos produtos deveriam vender mais:</strong></p>
<ul>
<li><strong>Verdadeiro.</strong> Quanto mais tempo uma loja fica com promoções extendidas, maior tende a ser a média de vendas. Entretanto, o mesmo não ocorre para as promoções regulares, que não aparentam influenciar nas vendas médias.</li>
</ul>
<p><img src="https://henriquelucasdf.github.io/images/h4.png" alt=""></p>
</li>
<li>
<p><strong>H5. Lojas com mais promoções consecutivas deveriam vender mais:</strong></p>
<ul>
<li><strong>Falso.</strong> Lojas que realizam apenas uma promoção vendem, em média, mais.</li>
</ul>
<p><img src="https://henriquelucasdf.github.io/images/h5.png" alt=""></p>
</li>
<li>
<p><strong>H6. Lojas abertas durante o Natal deveriam vender mais:</strong></p>
<ul>
<li><strong>Verdadeiro.</strong> As vendas médias no natal são maiores do que nos dias regulares. Em 2013 as vendas na páscoa foram as maiores, mas em 2014 o natal a ultrapassou.</li>
</ul>
<p><img src="https://henriquelucasdf.github.io/images/h6.png" alt=""></p>
</li>
<li>
<p><strong>H7. Lojas deveriam vender mais ao longo dos anos:</strong></p>
<ul>
<li><strong>Verdadeiro.</strong> A média de vendas vêm aumentando ao longo dos anos.</li>
</ul>
<p><img src="https://henriquelucasdf.github.io/images/h7.png" alt=""></p>
</li>
<li>
<p><strong>H8. Lojas deveriam vender mais no segundo semestre do ano:</strong></p>
<ul>
<li><strong>Verdadeiro.</strong> As vendas aumentam no final do ano.</li>
</ul>
<p><img src="https://henriquelucasdf.github.io/images/h8.png" alt=""></p>
</li>
<li>
<p><strong>H9. Lojas deveriam vender mais depois do dia 10 de cada mês:</strong></p>
<ul>
<li><strong>Falso.</strong> As vendas médias são levemente maiores antes do dia 10. Há uma leve tendência de queda nas vendas médias ao longo do mês.</li>
</ul>
<p><img src="https://henriquelucasdf.github.io/images/h9.png" alt=""></p>
</li>
<li>
<p><strong>H10. Lojas deveriam vender menos aos finais de semana:</strong></p>
<ul>
<li><strong>Parcialmente verdadeiro.</strong> As vendas médias são menores nos sábados, mas maiores nos domingos.</li>
</ul>
<p><img src="https://henriquelucasdf.github.io/images/h10.png" alt=""></p>
</li>
<li>
<p><strong>H11. Lojas deveriam vender menos durante os feriados escolares:</strong></p>
<ul>
<li><strong>Falso.</strong> Em média, as lojas vendem um pouco mais durante os feriados escolares.</li>
</ul>
<p><img src="https://henriquelucasdf.github.io/images/h11.png" alt=""></p>
</li>
</ul>
</li>
<li>
<p><strong>Resumo das hipóteses:</strong></p>
<p><img src="https://henriquelucasdf.github.io/images/resumo_h.png" alt=""></p>
</li>
</ul>
<h4 id="33-análise-multivariada">3.3 Análise Multivariada</h4>
<p>Nesta seção, busca-se responder duas questões:</p>
<ol>
<li>Existe alguma variável independente que possua uma alta correlação com a variável alvo? Se sim, esta feature é de extrema importância para o modelo.</li>
<li>Existe alguma variável independente que possua uma alta correlação com outra variável independente? Caso exista, é necessário retirar uma das duas, já que em modelos de machine learning um modelo mais simples sempre é preferível e, além disso, evita-se problemas de multicolinearidade.</li>
</ol>
<p>Para a realização da análise, o dataset foi dividido em colunas numéricas e em colunas categóricas. Para aquelas, será utilizado a correlação de Pearson, expondo os valores em um mapa de calor. Para estas, a técnica Cramer V será utilizada, através da biblioteca Dython.</p>
<ul>
<li>
<p><strong>Variáveis numéricas:</strong></p>
<p><img src="https://henriquelucasdf.github.io/images/corr_num.png" alt=""></p>
<ul>
<li>
<p>Percebe-se pelo gráfico acima que a variável dependente não é fortemente correlacionada com nenhuma das independentes. A coluna que apresenta maior correlação com a variável alvo é a coluna <strong>&ldquo;promo&rdquo;</strong> (correlação média), seguida pela coluna <strong>&ldquo;day_of_week&rdquo;</strong> (correlação fraca). Todas as outras colunas numéricas apresentam correlação fraca.</p>
</li>
<li>
<p>As seguintes variáveis podem apresentar problemas de multicolinearidade (alta correlação entre elas):</p>
<ul>
<li>competition_time_month x competition_open_since_year;</li>
<li>promo2_time_week x promo2_since_week</li>
<li>promo2 x promo2_since_year</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Variáveis Categóricas:</strong></p>
<p><img src="https://henriquelucasdf.github.io/images/corr_cat.png" alt=""></p>
<ul>
<li>store_type e assortment possuem uma correlação média, mas não devem causar problemas de multicolinearidade.</li>
</ul>
</li>
</ul>
<h2 id="40-preparação-dos-dados">4.0 Preparação dos dados:</h2>
<p>Nesta etapa serão realizadas 4 tarefas:</p>
<ol>
<li>Split dos dados entre treino e teste.</li>
<li>Rescaling dos dados numéricos.</li>
<li>Transformação cíclica dos dados temporais.</li>
<li>Encoding dos dados categóricos.</li>
</ol>
<h4 id="41-split-entre-treino-e-teste">4.1 Split entre treino e teste</h4>
<p>A primeira coisa a ser feita na preparação dos dados é a separação entre os dados de treino e os dados de teste. É vital que essa etapa ocorra antes das outras para que não ocorra vazamentos de dados e os resultados do modelo sejam enviesados.</p>
<p>Como iremos realizar a previsão de vendas para 6 semanas, iremos utilizar como dataset de teste os dados das últimas 6 semanas disponíveis. Esse período compreende as datas entre 19-06-2015 e 31-07-2015. A separação foi feita simplesmente por:</p>
<pre><code>      df_treino = df4[df4['date']&lt; '2015-06-19']
      df_teste  = df4[df4['date'] &gt;= '2015-06-19']
</code></pre>
<h4 id="42-rescaling-dos-dados-numéricos">4.2 Rescaling dos dados numéricos</h4>
<p>Para as variáveis independentes, dois tipos de scaler foram utilizados: <strong>MinMax scaler</strong> e <strong>Robust Scaler</strong>. O primeiro será utilizado para features sem muitos outliers, e o último será para aquelas que apresentarem alto número de itens outliers.</p>
<p>Podemos visualizar os outliers através dos BoxPlots abaixo:</p>
<p><img src="https://henriquelucasdf.github.io/images/scaling.png" alt=""></p>
<ul>
<li>
<p>Pode-se observar que <strong>competition_distance</strong> e <strong>competition_time_month</strong> possuem muitos outliers. Sendo assim, para estas variáveis o RobustScaler será utilizado.</p>
</li>
<li>
<p>Como <strong>year</strong> e <strong>promo_time_week</strong> possuem poucos outliers, para elas o MinMaxScaler será utilizado.</p>
</li>
<li>
<p>Uma observação importante é que deve-se fazer o rescaling tanto dos dados de treino como nos de teste. Entretanto, deve-se observar que o método <strong>.fit_transform</strong> deve ser aplicado ao treino e o <strong>.transform</strong> ao teste. Por exemplo:</p>
<pre><code>df_treino['competition_distance'] = rs.fit_transform(df_treino[['competition_distance']].values);
df_teste['competition_distance'] = rs.transform(df_teste[['competition_distance']].values);
</code></pre>
</li>
</ul>
<p>Além disso, uma boa prática na preparação dos dados para o modelo de Machine Learning é tentar aproximar a distribuição da variável resposta a uma distribuição normal, já que tais modelos lidam melhor com este tipo de distribuição. Um bom método para realizar esta tarefa é a <strong>Transformação Logarítmica</strong>, que realiza uma boa linearização dos valores, lida bem com grandes variâncias e torna a distribuição mais simétrica, diminuindo os resíduos.</p>
<p>Desta forma, a variável <strong>sales</strong> foi transformada pela função <strong>log1p</strong> da biblioteca numpy. Abaixo é possível visualizar a transformação.</p>
<p><img src="https://henriquelucasdf.github.io/images/sales_log.png" alt=""></p>
<h4 id="43-transformação-cíclica-dos-dados-temporais">4.3 Transformação cíclica dos dados temporais.</h4>
<p>Algumas variáveis temporais possuem uma natureza cíclica, como por exemplo, os meses do ano. O modelo de machine learning deve ser capaz de entender que o mês 12 é próximo do mês 1, assim como a semana 52 antecede a semana 1 do ano. Uma forma de implementar a natureza cíclica desses dados é através de coordenadas polares, transformando a variável em uma dupla de coordenadas trigonométricas. Um exemplo disso é visto abaixo:</p>
<pre><code>  df_treino['month_sin'] = df_treino['month'].apply(lambda x: np.sin(x*(2.*np.pi/12)))
  df_treino['month_cos'] = df_treino['month'].apply(lambda x: np.cos(x*(2.*np.pi/12)))
</code></pre>
<p>Desta forma, as variáveis <strong>month</strong>, <strong>day</strong>, <strong>week_of_year</strong> e <strong>day_of_week</strong> foram transformadas para novas variáveis polares, dividindo-as em seno e cosseno.</p>
<h4 id="44-encoding-de-variáveis-categóricas">4.4 Encoding de variáveis Categóricas</h4>
<ul>
<li>
<p><strong>One-Hot Encoding:</strong> Este tipo de encoding foi aplicado na variável <strong>state_holiday</strong>, já que os valores desta feature representam estados e não há uma ordenação entre eles.</p>
<ul>
<li>
<p>Deve-se atentar ao fato de que o encoding dos dados de treino e de teste devem ser realinhados, para que não haja estados com diferentes codificações. Isso pode ser feitos por:</p>
<pre><code>df_treino = pd.get_dummies(df_treino, prefix = ['state_holiday'], columns = ['state_holiday'])
df_teste =  pd.get_dummies(df_teste, prefix = ['state_holiday'], columns = ['state_holiday'])
# Realinhando os datasets:
df_treino, df_teste = df_treino.align(df_teste,join='left',axis=1)
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong>Label Encoder:</strong> Este encoding foi aplicado à feature <strong>store_type</strong>, pois seus valores não se relacionam ordinalmente entre si e nem representam estados.</p>
</li>
<li>
<p><strong>Ordinal Encoder:</strong> Já este foi aplicado para a variável <strong>assortment</strong>, pois existe claramente uma relação de hierarquia entre suas categorias.</p>
</li>
</ul>
<h2 id="50-seleção-de-features">5.0 Seleção de Features</h2>
<p>Nesta etapa do projeto, é necessário retirar do nosso dataset as variáveis independentes que são repetitivas ou que em nada acrescentam ao nosso modelo, possibilitando assim, a redução da sua dimensionalidade. Existem diversos métodos para realizar esta etapa do projeto, como os métodos de Filtro, os embutidos e os &ldquo;Wrappers&rdquo;. Ressalta-se que estes métodos servem apenas como um suporte ao projetista, já que restrições de negócio também devem ser consideradas ao se selecionar as features.</p>
<p>Neste projeto optou-se por utilizar um método Wrapper denominado <a href="https://pypi.org/project/Boruta/"><strong>Boruta</strong></a>, que seleciona automaticamente as melhores features para a modelo. Apesar de ser computacionalmente caro, o Boruta produz resultados superiores a outros métodos quando se trata de dados estruturados.</p>
<p>Inicialmente, foi realizado a retirada das variáveis temporais originais, já que estas foram convertidas para coordenadas polares em novas colunas. Assim, foram excluídas dos dados as colunas: <strong>week_of_year</strong>, <strong>day</strong>, <strong>month</strong>, <strong>day_of_week</strong>, <strong>promo_since</strong>, <strong>competition_since</strong> e <strong>year_week</strong>.</p>
<p>Com o algoritmo Boruta, as melhores features foram selecionadas. O modelo RandomForestRegressor foi escolhido para auxiliar do algoritmo.</p>
<p>O algoritmo retornou as seguintes features como relevantes:
<img src="https://henriquelucasdf.github.io/images/cols_selected.png" alt=""></p>
<p>Algumas considerações devem ser feitas:</p>
<ul>
<li>A features <strong>customers</strong> não pode entrar no modelo final, já que não é uma variável independente.</li>
<li>Deve-se inserir a feature <strong>date</strong> e a variável alvo, <strong>sales</strong>.</li>
<li>O algoritmo selecionou apenas a parte cossenoidal de algumas das variáveis temporais. É necessário inserir o par senoidal de cada uma delas.</li>
</ul>
<p>Com isso, a lista final de variáveis é mostrada abaixo:</p>
<p><img src="https://henriquelucasdf.github.io/images/cols_fin.png" alt=""></p>
<h2 id="60-elaboração-dos-modelos-de-machine-learning">6.0 Elaboração dos Modelos de Machine Learning</h2>
<p>Esta etapa foi dividida em 3 partes:</p>
<ol>
<li>Escolha das métricas de performance.</li>
<li>Escolha dos modelos de Machine Learning.</li>
<li>Cross-Validation dos modelos.</li>
</ol>
<h4 id="1-métricas-de-performance">1. Métricas de performance</h4>
<p>As seguintes métricas foram escolhidas para avaliar os resultados da modelagem:</p>
<ul>
<li>
<p><strong>Mean Absolute Error (MAE):</strong> O erro médio absoluto mostra quanto o modelo está errando na média. Sua unidade de medida é a mesma da previsão utilizada, em nosso caso, Dólar. Devido a isso, o MAE é uma boa métrica do ponto de vista do negócio já que demonstra imediatamente o impacto financeiro da predição, além de ser de fácil entendimento. Além disso, essa métrica é robusta a outliers, o que no nosso caso é algo positivo.</p>
</li>
<li>
<p><strong>Mean Absolute Percentage Error (MAPE):</strong> Esta métrica é análoga ao MAE, mas retornando valores de erros percentuais. A grande desvantagem do MAPE é que ele não pode ser utilizado quando temos valores nulos, o que não é nosso caso nesse projeto.</p>
</li>
<li>
<p><strong>Root Mean Squared Error (RMSE):</strong> O RMSE indica a raiz do erro quadrático médio, sendo uma métrica bastante sensível a outliers. Devido a isso,  o RMSE é uma excelente métrica para a performance do modelo de Machine Learning, sendo mais rigorosa com os erros do que o MAE/MAPE.</p>
</li>
<li>
<p><strong>Mean Percentage Error (MPE):</strong> o erro percentual médio é uma boa métrica para indicar se um modelo está sub ou superestimando as previsões realizadas. Valores positivos de MPE indica que o modelo está subestimando valores, enquanto valores negativos indicam uma superestimação.</p>
</li>
</ul>
<h4 id="2-escolha-dos-modelos-de-machine-learning">2. Escolha dos modelos de Machine Learning:</h4>
<p>Para realizar as predições, quatro modelos serão testados:</p>
<ul>
<li>
<p><strong>Regressão linear:</strong> o modelo de regressão linear é um modelo extremamente simples e será utilizado como um <strong>Baseline</strong> para nosso projeto. Dada a distribuição da variável alvo e das próprias conclusões da Análise Exploratória de Dados, é evidente que este modelo não apresentará resultados satisfatórios, sendo utilizado apenas como um guia para as outras implementações.</p>
</li>
<li>
<p><strong>Regressão Linear Regularizada (Lasso):</strong> o modelo Lasso é um avanço do modelo anterior, já que adiciona novos parâmetros capazes de realizar uma regularização da reta de regressão. Existem outros modelos de regressão regularizada, como o Ridge e o ElasticNet, mas nesse projeto vamos nos ater apenas ao Lasso.</p>
</li>
<li>
<p><strong>RandomForest Regressor:</strong> Este modelo é uma poderosa ferramenta tanto de classificação como de regressão. A RandomForest consiste em um Ensemble de múltiplas árvores de decisão através de uma agregação do tipo Bootstrap (Bagging), em que cada uma das árvores de decisão é treinada em diferentes amostras dos dados, e o resultado do modelo é um agrupamento do resultado de cada árvore.</p>
</li>
<li>
<p><strong>XGBoost Regressor:</strong> este modelo é uma implementação avançada do algoritmo Gradient Boosting, altamente sofisticado e capaz de lidar como todo tipo de irregularidade nos dados. O XGboost é um modelos fácil de usar, apesar de possuir diversos parâmetros que podem ser personalizados. Tal modelo é capaz de regularizar dados, possui alta flexibilidade, suporta processamentos paralelos, lida bem com valores NAs, entre outras vantagens.</p>
</li>
</ul>
<h4 id="3-cross-validation">3. Cross-Validation</h4>
<p>Para medir a real performance de um algoritmo de Machine Learning, é necessário realizar a cross-validation de nossos modelos. Este método consiste em um algoritmo que mede a performance dos modelos ao longo de toda a extensão dos dados, dividindo-o em subsets (k-folds). Um desses folds servirá como dados de validação, enquanto o restante dos dados será utilizado para treinar o modelo e, subsequentemente, realizar a previsão desejada, comparando-a com os dados do substet de validação. Após isso, outro subset é escolhido e o processo se repete, pelo número de vezes que o projetista achar necessário.</p>
<p>Neste projeto, a data foi utilizada como um indicador para os splits. No primeiro K-Fold, as últimas 6 semanas dos dados de treino foram utilizadas como validação, sendo o modelo treinado no restante dos dados. Para o segundo K-Fold, atrasamos mais 6 semanas, utilizando-as como validação e o restante como treino. Esse processo se repete até o K-Fold 5, número suficiente para avaliar a performance do modelo.</p>
<h3 id="61-performance-dos-modelos">6.1 Performance dos modelos:</h3>
<p>Abaixo é possível visualizar a performance de cada um dos quatro modelos utilizados:</p>
<p><img src="https://henriquelucasdf.github.io/images/modelos_cv.png" alt=""></p>
<p>Percebe-se que os modelos RandomForest e XGBoost apresentaram as melhores performances, com RMSEs de 1254.45 e 1455.64 na cross-validation, respectivamente. O pior modelo foi o LASSO, com RMSE de 3057.75.</p>
<p>Como o modelo XGBoost é capaz de treinar modelos muito mais rápido que a RandomForest, possui suporte para aceleração em GPU e apresentou resultados não muito diferentes, esse modelo será utilizado para realizar a previsão de vendas final.</p>
<h2 id="70-otimização-dos-hiperparâmetros">7.0 Otimização dos Hiperparâmetros</h2>
<p>Nesta etapa do projeto, o objetivo é encontrar os melhores parâmetros que maximizam a performance do nosso modelo XGBoost. Para realizar esta tarefa, um algoritmo de <strong>busca aleatória</strong> foi utilizado para minimizar o valor RMSE encontrado na cross-validation.</p>
<p>Os seguintes parâmetros foram otimizados:</p>
<ul>
<li><strong>n_estimators:</strong> É o número de árvores do modelo. Foi escolhido um range de 1000 a 3000 árvores para esse parâmetro.</li>
<li><strong>eta:</strong> É a taxa de aprendizado do modelo. Quanto menor melhor, pois previne o overfitting. Uma distribuição &ldquo;log-uniform&rdquo; de valores entre 0.001 a 0.03 foi atribuído para este parâmetro.</li>
<li><strong>max_depth:</strong> Define a profundidade máxima de uma árvore. Quanto mais profunda, mais complexo o modelo se torna, mas isso pode levar ao overfit. Um espaço uniforme de valores de 3 a 9 foram atribuídos para este item.</li>
<li><strong>subsample:</strong> É a porção de amostras de treino do modelo. É utilizado para evitar o overfitting. Neste parâmetro, valores entre 0.1 a 0.7 foram atribuídos.</li>
<li><strong>colsample_bytree:</strong> Representa a proporção de features para cada árvore. Optou-se por um range de 0.3 a 0.9.</li>
<li><strong>min_child_weight:</strong> É o peso necessário para criar um novo nó em uma árvore. Quanto menor, mais complexo se torna o modelo, mas tende a causar overfitting. Valores de 3 a 8 foram atribuídos.</li>
<li><strong>objective:</strong> define o método de aprendizado do modelo. <strong>&ldquo;reg:squarederror&rdquo;</strong> foi escolhido.</li>
<li><strong>tree_method:</strong> Permite a aceleração por GPU, através do valor <strong>&ldquo;gpu_hist&rdquo;</strong>.</li>
</ul>
<p>Este <a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">link</a> mostra detalhadamente como fazer a otimização dos hiperparâmetros no XGBoost, incluindo a indicação dos valores aqui adotados.</p>
<p>Após 100 interações realizadas, os melhores parâmetros encontrados para o modelo foram:</p>
<ul>
<li><strong>n_estimators:</strong> 1542</li>
<li><strong>eta:</strong> 0.0075</li>
<li><strong>max_depth:</strong> 9</li>
<li><strong>subsample:</strong> 0.6747</li>
<li><strong>colsample_bytree:</strong> 0.62</li>
<li><strong>min_child_weight:</strong> 7</li>
</ul>
<p>Com esses valores, o modelo apresentou as seguintes métricas para a cross-validation:
<img src="https://henriquelucasdf.github.io/images/tuned_model.png" alt=""></p>
<p>Sendo assim, pode-se observar uma redução de aproximadamente 20% no valor do RMSE e do MAPE, o que é um resultado excelente conseguido apenas com a otimização dos hiperparâmetros.</p>
<h2 id="80-validação-com-os-dados-de-teste">8.0 Validação com os dados de teste</h2>
<p>Com o modelo pronto, podemos realizar a validação final do projeto, utilizando agora os dados de teste separados no ínicio do item <a href="#40-prepara%C3%A7%C3%A3o-dos-dados">4.1</a>.</p>
<p>Realizando as previsões com o X_teste e comparando-as com o y_teste, obtemos:</p>
<p><img src="https://henriquelucasdf.github.io/images/teste_val.png" alt=""></p>
<p>A partir desse resultado, podemos concluir que o modelo projetado apresentou ótimos resultados, com baixos erros médios e previsões coerentes com a realidade. Além disso, como o MPE é positivo, podemos concluir que para este conjunto de dados, o modelo está subestimando os valores.</p>
<p>Iremos detalhar esta previsão final no próximo tópico, avaliando a performance do modelo para todas as lojas.</p>
<h3 id="81-performance-do-modelo-de-machine-learning">8.1 Performance do modelo de Machine Learning</h3>
<p>Pode-se visualizar os erros MAPE e RMSE para cada uma das lojas no gráfico de dispersão abaixo:</p>
<p><img src="https://henriquelucasdf.github.io/images/erro_teste.png" alt=""></p>
<p>Observa-se que para a grande maioria das lojas os erros apresentam valores satisfatórios, com valores abaixo de 20% para o MAPE e de 2000 para o RMSE. Entretanto, algumas das lojas apresentam grandes distorções, com MAPE acima do 50%. Para estas lojas, uma investigação mais aprofundada deveria ser realizada.</p>
<p>Pode-se ainda visualizar a dispersão dos erros:</p>
<p><img src="https://henriquelucasdf.github.io/images/disp_erro.png" alt=""></p>
<ul>
<li>
<p>No primeiro gráfico, podemos observar a diferença data a data entre as previsões e as vendas reais. Conclui-se que o modelo conseguiu captar bem o comportamento das vendas durante essas seis semanas, com as maiores distorções ocorrendo nas semanas com início no dia 8 e no meio da semana com início no dia 15.</p>
</li>
<li>
<p>O segundo gráfico, no canto superior direito, mostra a taxa de erro. Ou seja, a razão entre a previsão e o valor real. Pode-se observar que a maioria dos valores se situam abaixo dos 10% de erro, com outliers aparecendo nas semanas citadas anteriormente. Ressalta-se que o maior valor para a taxa de erro é de 0.85, o que é um valor razoável.</p>
</li>
<li>
<p>Considera-se como um bom modelo de Machine Learning aquele que produz resíduos normalmente distribuídos e com média nula. Observando o histograma dos erros no terceiro gráfico, vemos que o nosso modelo atende essa premissa, com grande parte dos valores centrados em zero e com um formato semelhante à gaussiana.</p>
</li>
<li>
<p>Já no último gráfico, podemos visualizar a dispersão do erro. Observa-se que os resíduos se concentram em formato tunelar, com erros entre +/- US$ 10.000,00. Apesar da presença de alguns outliers, podemos concluir que temos um ótimo modelo de machine learning.</p>
</li>
</ul>
<h3 id="82-performance-de-negócio">8.2 Performance de negócio</h3>
<p>Antes de colocar o modelo em produção, temos que organizar as previsões para cada loja. Desta forma, agrupamos as previsões de vendas das 6 semanas por loja, obtendo o valor final para cada uma delas. Além disso, obtemos os erros MAE e MAPE por loja e somamos e subtraímos o MAE das previsões realizadas, obtendo-se o melhor e o pior cenário, respectivamente. Exemplos das previsões podem ser visualizadas na tabela e no gráfico de dispersão abaixo.</p>
<p><img src="https://henriquelucasdf.github.io/images/tab_predi.png" alt="">
<img src="https://henriquelucasdf.github.io/images/predi.png" alt=""></p>
<p>Somando todas as previsões, temos uma previsão de vendas de US$ 283,881,088.00, sendo US$ 283,178,826.98 para o pior cenário e US$ 284,583,373.63 para o melhor cenário.</p>
<h2 id="90-deploy-do-modelo-em-produção">9.0 Deploy do modelo em produção</h2>
<ul class="pa0">
  
   <li class="list">
     <a href="https://henriquelucasdf.github.io/tags/previs%C3%A3o-de-vendas" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Previsão de Vendas</a>
   </li>
  
   <li class="list">
     <a href="https://henriquelucasdf.github.io/tags/modelo-em-produ%C3%A7%C3%A3o" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Modelo em Produção</a>
   </li>
  
   <li class="list">
     <a href="https://henriquelucasdf.github.io/tags/xgboost" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">XGBoost</a>
   </li>
  
   <li class="list">
     <a href="https://henriquelucasdf.github.io/tags/randomforest" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">RandomForest</a>
   </li>
  
   <li class="list">
     <a href="https://henriquelucasdf.github.io/tags/gpu-acceleration" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">GPU Acceleration</a>
   </li>
  
   <li class="list">
     <a href="https://henriquelucasdf.github.io/tags/bot-telegram" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Bot telegram</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://henriquelucasdf.github.io/" >
    &copy;  Henrique Lucas 2020 
  </a>
    <div>








<a href="https://github.com/henriquelucasdf" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>







</div>
  </div>
</footer>

    

  <script src="https://henriquelucasdf.github.io/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
